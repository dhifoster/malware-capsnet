{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment 3 - Baseline CapsNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu-8PnMq3l7V"
      },
      "source": [
        "# **MSc Data Science Project**\n",
        "\n",
        "## **Dhinta Foster (13156097)**\n",
        "\n",
        "The following notebook outlines the third experiment implementation of the baseline Capsule Neural Network. Please see references in the project report and throughout the notebook and on the README. In order to run any shell commands, please remove the \"!\" if not running on Google Colaboratory. The references below refer to the repositories from which the implementation was derived from. For the full reference list, please refer to the project report.\n",
        "\n",
        "**References**\n",
        "\n",
        "[1] Mallet, H (2020.). hugom1997/Malware_Classification. [online] Available at: https://github.com/hugom1997/Malware_Classification/blob/master/Malware_Classification.ipynb [Accessed 12 Sep. 2020].\n",
        "\n",
        "[2] Guo, X. (2017). XifengGuo/CapsNet-Keras. [online] Available at: https://github.com/XifengGuo/CapsNet-Keras/tree/tf2.2 [Accessed 12 Sep. 2020].\n",
        "\n",
        "â€Œ[3] Vyas, M. (2020). meenavyas/Misc. [online] Available at: https://github.com/meenavyas/Misc/blob/master/UCICreditCardKerasGridSearch.py [Accessed 12 Sep. 2020].\n",
        "\n",
        "[4] Dey, Ishaan. (n.d.). ishaandey/Classification_Evaluation_Walkthrough. [online] Available at: https://github.com/ishaandey/Classification_Evaluation_Walkthrough/blob/master/Classification_Evaluation.ipynb [Accessed 12 Sep. 2020]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFWL2Hbl3oSd"
      },
      "source": [
        "## Baseline Capsule Neural Network\n",
        "\n",
        "Sabour et al (2017)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bISbiCQN1gMD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db786e2e-2ec9-47e1-c784-02a9e59d968c"
      },
      "source": [
        "# Imports\n",
        "\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoOGL_0GTw_1"
      },
      "source": [
        "##Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld3sUuPl3sUF"
      },
      "source": [
        "# For reproducibility, set seed\n",
        "\n",
        "np.random.seed(404)\n",
        "tf.random.set_seed(404) # note new notation for 2.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho-ThvQMV9Vk"
      },
      "source": [
        "##Data Load & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hjc-pzqV66-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0611b83c-73cb-4377-ec98-cfba1c42689d"
      },
      "source": [
        "# Load MalIMG data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPeSMYIAW11r"
      },
      "source": [
        "zip_path = '/content/drive/My Drive/malimg_dataset.zip'\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q \"malimg_dataset.zip\" \n",
        "!rm \"malimg_dataset.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyPtHIcqW2nw"
      },
      "source": [
        "root_path = \"/content/malimg_paper_dataset_imgs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxRBE6k6W4VN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6ab6d55-b5af-4697-eac9-85c89992618a"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "batches = ImageDataGenerator().flow_from_directory(directory=root_path, target_size=(64,64), batch_size=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9339 images belonging to 25 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLC3KMBEW5rC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "67c82f3e-f71c-43ef-8218-775d31ff8fef"
      },
      "source": [
        "batches.class_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Adialer.C': 0,\n",
              " 'Agent.FYI': 1,\n",
              " 'Allaple.A': 2,\n",
              " 'Allaple.L': 3,\n",
              " 'Alueron.gen!J': 4,\n",
              " 'Autorun.K': 5,\n",
              " 'C2LOP.P': 6,\n",
              " 'C2LOP.gen!g': 7,\n",
              " 'Dialplatform.B': 8,\n",
              " 'Dontovo.A': 9,\n",
              " 'Fakerean': 10,\n",
              " 'Instantaccess': 11,\n",
              " 'Lolyda.AA1': 12,\n",
              " 'Lolyda.AA2': 13,\n",
              " 'Lolyda.AA3': 14,\n",
              " 'Lolyda.AT': 15,\n",
              " 'Malex.gen!J': 16,\n",
              " 'Obfuscator.AD': 17,\n",
              " 'Rbot!gen': 18,\n",
              " 'Skintrim.N': 19,\n",
              " 'Swizzor.gen!E': 20,\n",
              " 'Swizzor.gen!I': 21,\n",
              " 'VB.AT': 22,\n",
              " 'Wintrim.BX': 23,\n",
              " 'Yuner.A': 24}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9XpJqP4W60x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5a6c0ea5-815b-441e-f18c-a9e3380fed3f"
      },
      "source": [
        "imgs, labels = next(batches)\n",
        "print(labels.shape)\n",
        "print(imgs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9339, 25)\n",
            "(9339, 64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-JBspfiXCLm"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(imgs/255.,labels, train_size=7004, test_size=1133)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b483V9NaXDD1"
      },
      "source": [
        "##Implement Capsule Layers\n",
        "\n",
        "The following classes are derived from Xifeng Guo's (2017) tensorflow implementation and updated to tf version 2.3 (Please see: https://github.com/XifengGuo/CapsNet-Keras/tree/tf2.2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34clw9RaXEGM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f48e4b4-d709-457a-80bd-3d14f5b51d52"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import initializers, layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n4zezGxXFoR"
      },
      "source": [
        "# The following classes are required to compute parts of the capsule\n",
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
        "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
        "    inputs: shape=[None, num_vectors, dim_vector]\n",
        "    output: shape=[None, num_vectors]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), -1) + K.epsilon())\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Length, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
        "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
        "    masked Tensor.\n",
        "        ```\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = tf.sqrt(tf.reduce_sum(tf.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = tf.one_hot(indices=tf.argmax(x, 1), depth=x.shape[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        masked = K.batch_flatten(inputs * tf.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:  # true label provided\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Mask, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        # Transform matrix, from each input capsule to each output capsule, there's a unique weight as in Dense layer.\n",
        "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
        "                                        self.dim_capsule, self.input_dim_capsule],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule, 1]\n",
        "        inputs_expand = tf.expand_dims(tf.expand_dims(inputs, 1), -1)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
        "        inputs_tiled = tf.tile(inputs_expand, [1, self.num_capsule, 1, 1, 1])\n",
        "\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
        "        # Regard the first two dimensions as `batch` dimension, then\n",
        "        # matmul(W, x): [..., dim_capsule, input_dim_capsule] x [..., input_dim_capsule, 1] -> [..., dim_capsule, 1].\n",
        "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "        inputs_hat = tf.squeeze(tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled))\n",
        "\n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, 1, self.input_num_capsule].\n",
        "        b = tf.zeros(shape=[inputs.shape[0], self.num_capsule, 1, self.input_num_capsule])\n",
        "\n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            # c.shape=[batch_size, num_capsule, 1, input_num_capsule]\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "\n",
        "            # c.shape = [batch_size, num_capsule, 1, input_num_capsule]\n",
        "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "            # The first two dimensions as `batch` dimension,\n",
        "            # then matmal: [..., 1, input_num_capsule] x [..., input_num_capsule, dim_capsule] -> [..., 1, dim_capsule].\n",
        "            # outputs.shape=[None, num_capsule, 1, dim_capsule]\n",
        "            outputs = squash(tf.matmul(c, inputs_hat))  # [None, 10, 1, 16]\n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, num_capsule, 1, dim_capsule]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "                # The first two dimensions as `batch` dimension, then\n",
        "                # matmal:[..., 1, dim_capsule] x [..., input_num_capsule, dim_capsule]^T -> [..., 1, input_num_capsule].\n",
        "                # b.shape=[batch_size, num_capsule, 1, input_num_capsule]\n",
        "                b += tf.matmul(outputs, inputs_hat, transpose_b=True)\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "\n",
        "        return tf.squeeze(outputs)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "    \"\"\"\n",
        "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_capsule: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
        "    \"\"\"\n",
        "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
        "                           name='primarycap_conv2d')(inputs)\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
        "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFKrUNT9XJH-"
      },
      "source": [
        "# Define the margin loss function\n",
        "\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    # return tf.reduce_mean(tf.square(y_pred))\n",
        "    L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return tf.reduce_mean(tf.reduce_sum(L, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-X3h9-RXL6O"
      },
      "source": [
        "## Build the baseline CapsNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ1AMgT8XLAU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "509db26a-472c-4a03-e675-ae662c424b04"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Input, Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc27lhpeXPGh"
      },
      "source": [
        "num_classes = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTdMdhggXQo5"
      },
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "def CapsNet(input_shape, n_class, routings, batch_size):\n",
        "    \"\"\"\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :param batch_size: size of batch\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    x = layers.Input(shape=input_shape, batch_size=batch_size) # MalIMG input shape is 64*64*3\n",
        "\n",
        "    # Layer 1-4: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "    \n",
        "    # The below is the cayir implementation (add when needed)\n",
        "    #conv1 = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='valid', activation='relu', name='conv1')(x)\n",
        "    #conv3 = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='valid', activation='relu', name='conv1')(x)\n",
        "    #conv2 = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='valid', activation='relu', name='conv1')(x)\n",
        "    #conv4 = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='valid', activation='relu', name='conv1')(x)\n",
        "    #conv5 = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    #conv6 = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "    # Layer 5: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "    primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=3, padding='valid') \n",
        "\n",
        "    # Layer 6: Capsule layer. Routing algorithm works here.\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings, name='digitcaps')(primarycaps)\n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='capsnet')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    y = layers.Input(shape=(n_class,))\n",
        "    masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # Shared Decoder model in training and prediction\n",
        "    decoder = models.Sequential(name='decoder')\n",
        "    decoder.add(layers.Dense(512, activation='relu', input_dim=16 * n_class))\n",
        "    decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "\n",
        "    # manipulate model\n",
        "    noise = layers.Input(shape=(n_class, 16))\n",
        "    noised_digitcaps = layers.Add()([digitcaps, noise])\n",
        "    masked_noised_y = Mask()([noised_digitcaps, y])\n",
        "    manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
        "    return train_model, eval_model, manipulate_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0J01NmWXS-P"
      },
      "source": [
        "# The following section of code is adapted in order to fit the MalIMG dataset\n",
        "\n",
        "def train(model, data, args):\n",
        "    \"\"\"\n",
        "    Training a CapsuleNet\n",
        "    :param model: the CapsuleNet model\n",
        "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
        "    :param args: arguments\n",
        "    :return: The trained model\n",
        "    \"\"\"\n",
        "    # unpacking the data\n",
        "    (X_train, y_train), (X_test, y_test) = data # Please ensure MalIMG dataset is already split.\n",
        "\n",
        "    # callbacks\n",
        "    log = callbacks.CSVLogger(args.save_dir + '/log.csv')\n",
        "    checkpoint = callbacks.ModelCheckpoint(args.save_dir + '/weights-{epoch:02d}.h5', monitor='capsnet_accuracy',\n",
        "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
        "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (args.lr_decay ** epoch))\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer=optimizers.Adam(lr=args.lr),\n",
        "                  loss=[margin_loss, 'mse'],\n",
        "                  loss_weights=[1., args.lam_recon],\n",
        "                  metrics={'capsnet': 'accuracy'})\n",
        "\n",
        "    \n",
        "    # Training without data augmentation:\n",
        "    model.fit([X_train, y_train], [y_train, X_train], batch_size=args.batch_size, epochs=args.epochs,\n",
        "              validation_data=[[X_test, y_test], [y_test, X_test]], callbacks=[log, checkpoint, lr_decay])\n",
        "\n",
        "    model.save_weights(args.save_dir + '/trained_model.h5')\n",
        "    print('Trained model saved to \\'%s/trained_model.h5\\'' % args.save_dir)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eEAt9qgYH6P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e568093-f079-4dbc-89a3-25d21bda554d"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7004, 25)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6h5ZviyYJ05"
      },
      "source": [
        "y_train_new = np.argmax(y_train, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbOmvpasYLab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7b87a61-f9fe-4ab7-a7a9-4a9cfd0c89d1"
      },
      "source": [
        "y_train_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 2, 3, ..., 3, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsIjUng7YOU3"
      },
      "source": [
        "# class_weight function cannot deal with one hot encoded y. We need to convert it.\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(y_train_new),\n",
        "                                                 y_train_new)\n",
        "class_weights = {i : class_weights[i] for i in range(25)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTItbhZOYO1r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "c229f762-569d-422b-b64b-3c28063e06ba"
      },
      "source": [
        "class_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 3.0786813186813187,\n",
              " 1: 2.980425531914894,\n",
              " 2: 0.1279269406392694,\n",
              " 3: 0.23503355704697987,\n",
              " 4: 1.959160839160839,\n",
              " 5: 3.686315789473684,\n",
              " 6: 2.523963963963964,\n",
              " 7: 1.9058503401360545,\n",
              " 8: 2.18875,\n",
              " 9: 2.4361739130434783,\n",
              " 10: 0.9899646643109541,\n",
              " 11: 0.8646913580246913,\n",
              " 12: 1.6776047904191618,\n",
              " 13: 2.0752592592592594,\n",
              " 14: 2.8882474226804122,\n",
              " 15: 2.457543859649123,\n",
              " 16: 2.4361739130434783,\n",
              " 17: 2.6183177570093457,\n",
              " 18: 2.2593548387096773,\n",
              " 19: 4.669333333333333,\n",
              " 20: 2.9183333333333334,\n",
              " 21: 2.6430188679245283,\n",
              " 22: 0.9037419354838709,\n",
              " 23: 4.0602898550724635,\n",
              " 24: 0.45927868852459014}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NcEJIbqXVXP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bc8067c0-b234-4ead-f68e-58a8a0c93883"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Capsule Network on MalIMG.\")\n",
        "parser.add_argument('--epochs', default=10, type=int)\n",
        "parser.add_argument('--batch_size', default=103, type=int) #batch size changed to fit MalIMG dataset\n",
        "parser.add_argument('--lr', default=0.01, type=float,\n",
        "                        help=\"Initial learning rate\")\n",
        "parser.add_argument('--lr_decay', default=0.9, type=float,\n",
        "                        help=\"The value multiplied by lr at each epoch. Set a larger value for larger epochs\")\n",
        "parser.add_argument('--lam_recon', default=0.392, type=float,\n",
        "                        help=\"The coefficient for the loss of decoder\")\n",
        "parser.add_argument('-r', '--routings', default=3, type=int,\n",
        "                        help=\"Number of iterations used in routing algorithm. should > 0\")\n",
        "parser.add_argument('--shift_fraction', default=0.1, type=float,\n",
        "                        help=\"Fraction of pixels to shift at most in each direction.\")\n",
        "parser.add_argument('--debug', action='store_true',\n",
        "                        help=\"Save weights by TensorBoard\")\n",
        "parser.add_argument('--save_dir', default='./result')\n",
        "parser.add_argument('-t', '--testing', action='store_true',\n",
        "                        help=\"Test the trained model on testing dataset\")\n",
        "parser.add_argument('--digit', default=5, type=int,\n",
        "                        help=\"Digit to manipulate\")\n",
        "parser.add_argument('-w', '--weights', default=None,\n",
        "                        help=\"The path of the saved weights. Should be specified when testing\")\n",
        "args, unknown = parser.parse_known_args()\n",
        "print(args)\n",
        "\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs(args.save_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=103, debug=False, digit=5, epochs=10, lam_recon=0.392, lr=0.01, lr_decay=0.9, routings=3, save_dir='./result', shift_fraction=0.1, testing=False, weights=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N48NvQ-aXXUS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "6571f832-3dc8-4598-acbb-6bf703fa43a3"
      },
      "source": [
        "# define model\n",
        "model, eval_model, manipulate_model = CapsNet(input_shape=X_train.shape[1:],\n",
        "                                                  n_class=len(np.unique(np.argmax(y_train, 1))),\n",
        "                                                  routings=args.routings,\n",
        "                                                  batch_size=args.batch_size)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(103, 64, 64, 3)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (103, 56, 56, 256)   62464       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_conv2d (Conv2D)      (103, 16, 16, 256)   5308672     conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_reshape (Reshape)    (103, 8192, 8)       0           primarycap_conv2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_squash (Lambda)      (103, 8192, 8)       0           primarycap_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "digitcaps (CapsuleLayer)        (103, 25, 16)        26214400    primarycap_squash[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 25)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "mask (Mask)                     (103, 400)           0           digitcaps[0][0]                  \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "capsnet (Length)                (103, 25)            0           digitcaps[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Sequential)            (None, 64, 64, 3)    13325824    mask[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 44,911,360\n",
            "Trainable params: 44,911,360\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BtTkxgSXaIu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "e806d3b0-f1df-4592-96c2-f3c9c13384e3"
      },
      "source": [
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "    tf.keras.callbacks.History(),\n",
        "]\n",
        "\n",
        "history = train(model=model, data=((X_train, y_train), (X_test, y_test)), args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.6208 - capsnet_loss: 1.5890 - decoder_loss: 0.0813 - capsnet_accuracy: 0.0398\n",
            "Epoch 00001: capsnet_accuracy improved from -inf to 0.03983, saving model to ./result/weights-01.h5\n",
            "68/68 [==============================] - 140s 2s/step - loss: 1.6208 - capsnet_loss: 1.5890 - decoder_loss: 0.0813 - capsnet_accuracy: 0.0398 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.5594 - capsnet_loss: 1.5289 - decoder_loss: 0.0777 - capsnet_accuracy: 0.0393\n",
            "Epoch 00002: capsnet_accuracy did not improve from 0.03983\n",
            "68/68 [==============================] - 143s 2s/step - loss: 1.5594 - capsnet_loss: 1.5289 - decoder_loss: 0.0777 - capsnet_accuracy: 0.0393 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.4472 - capsnet_loss: 1.4177 - decoder_loss: 0.0753 - capsnet_accuracy: 0.0425\n",
            "Epoch 00003: capsnet_accuracy improved from 0.03983 to 0.04255, saving model to ./result/weights-03.h5\n",
            "68/68 [==============================] - 144s 2s/step - loss: 1.4472 - capsnet_loss: 1.4177 - decoder_loss: 0.0753 - capsnet_accuracy: 0.0425 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.3413 - capsnet_loss: 1.3127 - decoder_loss: 0.0729 - capsnet_accuracy: 0.0373\n",
            "Epoch 00004: capsnet_accuracy did not improve from 0.04255\n",
            "68/68 [==============================] - 144s 2s/step - loss: 1.3413 - capsnet_loss: 1.3127 - decoder_loss: 0.0729 - capsnet_accuracy: 0.0373 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.2897 - capsnet_loss: 1.2614 - decoder_loss: 0.0722 - capsnet_accuracy: 0.0414\n",
            "Epoch 00005: capsnet_accuracy did not improve from 0.04255\n",
            "68/68 [==============================] - 144s 2s/step - loss: 1.2897 - capsnet_loss: 1.2614 - decoder_loss: 0.0722 - capsnet_accuracy: 0.0414 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.2502 - capsnet_loss: 1.2220 - decoder_loss: 0.0719 - capsnet_accuracy: 0.0393\n",
            "Epoch 00006: capsnet_accuracy did not improve from 0.04255\n",
            "68/68 [==============================] - 144s 2s/step - loss: 1.2502 - capsnet_loss: 1.2220 - decoder_loss: 0.0719 - capsnet_accuracy: 0.0393 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.2250 - capsnet_loss: 1.1969 - decoder_loss: 0.0717 - capsnet_accuracy: 0.0387\n",
            "Epoch 00007: capsnet_accuracy did not improve from 0.04255\n",
            "68/68 [==============================] - 144s 2s/step - loss: 1.2250 - capsnet_loss: 1.1969 - decoder_loss: 0.0717 - capsnet_accuracy: 0.0387 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.2124 - capsnet_loss: 1.1845 - decoder_loss: 0.0713 - capsnet_accuracy: 0.0404\n",
            "Epoch 00008: capsnet_accuracy did not improve from 0.04255\n",
            "68/68 [==============================] - 144s 2s/step - loss: 1.2124 - capsnet_loss: 1.1845 - decoder_loss: 0.0713 - capsnet_accuracy: 0.0404 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.2056 - capsnet_loss: 1.1780 - decoder_loss: 0.0703 - capsnet_accuracy: 0.0358\n",
            "Epoch 00009: capsnet_accuracy did not improve from 0.04255\n",
            "68/68 [==============================] - 144s 2s/step - loss: 1.2056 - capsnet_loss: 1.1780 - decoder_loss: 0.0703 - capsnet_accuracy: 0.0358 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "68/68 [==============================] - ETA: 0s - loss: 1.1982 - capsnet_loss: 1.1712 - decoder_loss: 0.0690 - capsnet_accuracy: 0.0403\n",
            "Epoch 00010: capsnet_accuracy did not improve from 0.04255\n",
            "68/68 [==============================] - 144s 2s/step - loss: 1.1982 - capsnet_loss: 1.1712 - decoder_loss: 0.0690 - capsnet_accuracy: 0.0403 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Trained model saved to './result/trained_model.h5'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE0nv9skllJ_"
      },
      "source": [
        "def test(model, data, args):\n",
        "    X_test, y_test = data\n",
        "    y_pred, x_recon = model.predict(X_test, batch_size=103)\n",
        "    print('-' * 30 + 'Begin: test' + '-' * 30)\n",
        "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1)) / y_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svw-Zr2cs-Wi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fbc37e1f-8c9c-4ede-ac35-1e44597d30b5"
      },
      "source": [
        "test(model=eval_model, data=(X_test, y_test), args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------Begin: test------------------------------\n",
            "Test acc: 0.0353045013239188\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}