{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment 4 - Malware CapsNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPgy70ZH8a9O"
      },
      "source": [
        "# **MSc Data Science Project**\n",
        "\n",
        "## **Dhinta Foster (13156097)**\n",
        "\n",
        "The following notebook outlines the fourth experiment implementation of a Capsule Neural Network adapted for the Malimg DATASET. Please see references in the project report and throughout the notebook and on the README. In order to run any shell commands, please remove the \"!\" if not running on Google Colaboratory. The references below refer to the repositories from which the implementation was derived from. For the full reference list, please refer to the project report.\n",
        "\n",
        "**References**\n",
        "\n",
        "[1] Mallet, H (2020.). hugom1997/Malware_Classification. [online] Available at: https://github.com/hugom1997/Malware_Classification/blob/master/Malware_Classification.ipynb [Accessed 12 Sep. 2020].\n",
        "\n",
        "[2] Guo, X. (2017). XifengGuo/CapsNet-Keras. [online] Available at: https://github.com/XifengGuo/CapsNet-Keras/tree/tf2.2 [Accessed 12 Sep. 2020].\n",
        "\n",
        "â€Œ[3] Vyas, M. (2020). meenavyas/Misc. [online] Available at: https://github.com/meenavyas/Misc/blob/master/UCICreditCardKerasGridSearch.py [Accessed 12 Sep. 2020].\n",
        "\n",
        "[4] Dey, Ishaan. (n.d.). ishaandey/Classification_Evaluation_Walkthrough. [online] Available at: https://github.com/ishaandey/Classification_Evaluation_Walkthrough/blob/master/Classification_Evaluation.ipynb [Accessed 12 Sep. 2020]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xMpFS398bVY"
      },
      "source": [
        "## Adapted Malware Capsule Neural Network\n",
        "\n",
        "Sabour et al (2017)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gbrVTVBtT75"
      },
      "source": [
        "# Imports\n",
        "\n",
        "from __future__ import division, print_function, unicode_literals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9qlAos7t5lg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48905731-f4a8-462d-a7df-5bfd8f5dc4d5"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA1bfQZ1t648"
      },
      "source": [
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDNJW6hRt8VZ"
      },
      "source": [
        "# For reproducibility, set seed\n",
        "\n",
        "np.random.seed(404)\n",
        "tf.random.set_seed(404) # note new notation for 2.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJKwsh2y_THj"
      },
      "source": [
        "## Load & Pre-process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcwEaOYnt-pL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ce5e844-d04f-4e3e-e672-ebfa583a3160"
      },
      "source": [
        "# Load MalIMG data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-ViFe_nt_Gu"
      },
      "source": [
        "zip_path = '/content/drive/My Drive/malimg_dataset.zip'\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q \"malimg_dataset.zip\" \n",
        "!rm \"malimg_dataset.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJjA9g_nuAll"
      },
      "source": [
        "root_path = \"/content/malimg_paper_dataset_imgs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIshk8uBuB_e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff125610-195a-4431-bd64-b70061fb2bb0"
      },
      "source": [
        "# Thank you to https://github.com/hugom1997/Malware_Classification/blob/master/Malware_Classification.ipynb for the repository with the data preprocessing\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "batches = ImageDataGenerator().flow_from_directory(directory=root_path, target_size=(64,64), batch_size=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9339 images belonging to 25 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwsVIAeMuDX-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "86718145-b580-405e-8937-65ceedbffc88"
      },
      "source": [
        "batches.class_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Adialer.C': 0,\n",
              " 'Agent.FYI': 1,\n",
              " 'Allaple.A': 2,\n",
              " 'Allaple.L': 3,\n",
              " 'Alueron.gen!J': 4,\n",
              " 'Autorun.K': 5,\n",
              " 'C2LOP.P': 6,\n",
              " 'C2LOP.gen!g': 7,\n",
              " 'Dialplatform.B': 8,\n",
              " 'Dontovo.A': 9,\n",
              " 'Fakerean': 10,\n",
              " 'Instantaccess': 11,\n",
              " 'Lolyda.AA1': 12,\n",
              " 'Lolyda.AA2': 13,\n",
              " 'Lolyda.AA3': 14,\n",
              " 'Lolyda.AT': 15,\n",
              " 'Malex.gen!J': 16,\n",
              " 'Obfuscator.AD': 17,\n",
              " 'Rbot!gen': 18,\n",
              " 'Skintrim.N': 19,\n",
              " 'Swizzor.gen!E': 20,\n",
              " 'Swizzor.gen!I': 21,\n",
              " 'VB.AT': 22,\n",
              " 'Wintrim.BX': 23,\n",
              " 'Yuner.A': 24}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lztrTi8kuEpo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5664cde6-6ad2-4b3c-bb1a-7c80892ff4c7"
      },
      "source": [
        "imgs, labels = next(batches)\n",
        "labels.shape\n",
        "imgs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9339, 64, 64, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3vVtIMbuGDk"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(imgs/255.,labels, train_size=7004, test_size=1133)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ-4VWqduKez",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c34132bc-772e-451e-833d-5cc0863da4b4"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import initializers, layers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIXn0dQ2_aWe"
      },
      "source": [
        "##Implement Capsule Layers\n",
        "\n",
        "The following classes are derived from Xifeng Guo's tensorflow implementation and updated to tf version 2.3 (Please see: https://github.com/XifengGuo/CapsNet-Keras/tree/tf2.2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-y5uAJJuMgw"
      },
      "source": [
        "# The following classes are required to compute parts of the capsule\n",
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
        "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
        "    inputs: shape=[None, num_vectors, dim_vector]\n",
        "    output: shape=[None, num_vectors]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), -1) + K.epsilon())\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Length, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
        "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
        "    masked Tensor.\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = tf.sqrt(tf.reduce_sum(tf.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = tf.one_hot(indices=tf.argmax(x, 1), depth=x.shape[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        masked = K.batch_flatten(inputs * tf.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:  # true label provided\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Mask, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        # Transform matrix, from each input capsule to each output capsule, there's a unique weight as in Dense layer.\n",
        "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
        "                                        self.dim_capsule, self.input_dim_capsule],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule, 1]\n",
        "        inputs_expand = tf.expand_dims(tf.expand_dims(inputs, 1), -1)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
        "        inputs_tiled = tf.tile(inputs_expand, [1, self.num_capsule, 1, 1, 1])\n",
        "\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
        "        # Regard the first two dimensions as `batch` dimension, then\n",
        "        # matmul(W, x): [..., dim_capsule, input_dim_capsule] x [..., input_dim_capsule, 1] -> [..., dim_capsule, 1].\n",
        "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "        inputs_hat = tf.squeeze(tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled))\n",
        "\n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, 1, self.input_num_capsule].\n",
        "        b = tf.zeros(shape=[inputs.shape[0], self.num_capsule, 1, self.input_num_capsule])\n",
        "\n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            # c.shape=[batch_size, num_capsule, 1, input_num_capsule]\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "\n",
        "            # c.shape = [batch_size, num_capsule, 1, input_num_capsule]\n",
        "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "            # The first two dimensions as `batch` dimension,\n",
        "            # then matmal: [..., 1, input_num_capsule] x [..., input_num_capsule, dim_capsule] -> [..., 1, dim_capsule].\n",
        "            # outputs.shape=[None, num_capsule, 1, dim_capsule]\n",
        "            outputs = squash(tf.matmul(c, inputs_hat))  # [None, 10, 1, 16]\n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, num_capsule, 1, dim_capsule]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "                # The first two dimensions as `batch` dimension, then\n",
        "                # matmal:[..., 1, dim_capsule] x [..., input_num_capsule, dim_capsule]^T -> [..., 1, input_num_capsule].\n",
        "                # b.shape=[batch_size, num_capsule, 1, input_num_capsule]\n",
        "                b += tf.matmul(outputs, inputs_hat, transpose_b=True)\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "\n",
        "        return tf.squeeze(outputs)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "    \"\"\"\n",
        "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_capsule: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
        "    \"\"\"\n",
        "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
        "                           name='primarycap_conv2d')(inputs)\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
        "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwY_P6M4uPoO"
      },
      "source": [
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    # return tf.reduce_mean(tf.square(y_pred))\n",
        "    L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return tf.reduce_mean(tf.reduce_sum(L, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjcLD40-uReX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7deeb229-1d15-43a7-a346-37357145b1c1"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Input, Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jQSLMxIuUNS"
      },
      "source": [
        "num_classes = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rDE2vUiuWvq"
      },
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "def CapsNet(input_shape, n_class, routings, batch_size):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :param batch_size: size of batch\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    x = layers.Input(shape=input_shape, batch_size=batch_size) # our input shape is 64*64*3\n",
        "\n",
        "    # Layer 1-4: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=5, kernel_size=3, strides=2, padding='valid', activation='relu', name='conv1')(x)\n",
        "    conv2 = layers.Conv2D(filters=5, kernel_size=3, strides=2, padding='valid', activation='relu', name='conv2')(x)\n",
        "    conv3 = layers.Conv2D(filters=25, kernel_size=3, strides=2, padding='valid', activation='relu', name='conv3')(x)\n",
        "    conv4 = layers.Conv2D(filters=25, kernel_size=3, strides=2, padding='valid', activation='relu', name='conv4')(x)\n",
        "    conv5 = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    #conv6 = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "    # Layer 5: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "    primarycaps = PrimaryCap(conv5, dim_capsule=8, n_channels=32, kernel_size=3, strides=3, padding='valid') \n",
        "\n",
        "    # Layer 6: Capsule layer. Routing algorithm works here.\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings, name='digitcaps')(primarycaps)\n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='capsnet')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    y = layers.Input(shape=(n_class,))\n",
        "    masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # Shared Decoder model in training and prediction\n",
        "    decoder = models.Sequential(name='decoder')\n",
        "    decoder.add(layers.Dense(512, activation='relu', input_dim=16 * n_class))\n",
        "    decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "\n",
        "    # manipulate model\n",
        "    noise = layers.Input(shape=(n_class, 16))\n",
        "    noised_digitcaps = layers.Add()([digitcaps, noise])\n",
        "    masked_noised_y = Mask()([noised_digitcaps, y])\n",
        "    manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
        "    return train_model, eval_model, manipulate_model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Wfg1x0TuXXJ"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import csv\n",
        "import math\n",
        "import pandas\n",
        "\n",
        "def combine_images(generated_images, height=None, width=None):\n",
        "    num = generated_images.shape[0]\n",
        "    if width is None and height is None:\n",
        "        width = int(math.sqrt(num))\n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif width is not None and height is None:  # height not given\n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif height is not None and width is None:  # width not given\n",
        "        width = int(math.ceil(float(num)/height))\n",
        "\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image\n",
        "\n",
        "def plot_log(filename, show=True):\n",
        "\n",
        "    data = pandas.read_csv(filename)\n",
        "\n",
        "    fig = plt.figure(figsize=(4,6))\n",
        "    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\n",
        "    fig.add_subplot(211)\n",
        "    for key in data.keys():\n",
        "        if key.find('loss') >= 0 and not key.find('val') >= 0:  # training loss\n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training loss')\n",
        "\n",
        "    fig.add_subplot(212)\n",
        "    for key in data.keys():\n",
        "        if key.find('acc') >= 0:  # acc\n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training and validation accuracy')\n",
        "\n",
        "    # fig.savefig('result/log.png')\n",
        "    if show:\n",
        "        plt.show()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_teSAJQuZiP"
      },
      "source": [
        "# The following section of code is adapted in order to fit the MalIMG dataset\n",
        "\n",
        "def train(model, data, args):\n",
        "    \"\"\"\n",
        "    Training a CapsuleNet\n",
        "    :param model: the CapsuleNet model\n",
        "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
        "    :param args: arguments\n",
        "    :return: The trained model\n",
        "    \"\"\"\n",
        "    # unpacking the data\n",
        "    (X_train, y_train), (X_test, y_test) = data # Please ensure MalIMG dataset is already split.\n",
        "\n",
        "    # callbacks\n",
        "    log = callbacks.CSVLogger(args.save_dir + '/log.csv')\n",
        "    checkpoint = callbacks.ModelCheckpoint(args.save_dir + '/weights-{epoch:02d}.h5', monitor='capsnet_accuracy',\n",
        "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
        "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (args.lr_decay ** epoch))\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer=optimizers.Adam(lr=args.lr),\n",
        "                  loss=[margin_loss, 'mse'],\n",
        "                  loss_weights=[1., args.lam_recon],\n",
        "                  metrics={'capsnet': 'accuracy'})\n",
        "\n",
        "    \n",
        "    # Training without data augmentation:\n",
        "    model.fit([X_train, y_train], [y_train, X_train], batch_size=args.batch_size, epochs=args.epochs,\n",
        "              validation_data=[[X_test, y_test], [y_test, X_test]], callbacks=[log, checkpoint, lr_decay])\n",
        "  \n",
        "\n",
        "    model.save_weights(args.save_dir + '/trained_model.h5')\n",
        "    print('Trained model saved to \\'%s/trained_model.h5\\'' % args.save_dir)\n",
        "\n",
        "    plot_log(args.save_dir + '/log.csv', show=True)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG-3HbtVubeF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45aee5e1-26e0-4f73-f2a1-7a8002178e48"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Capsule Network on Malimg.\")\n",
        "parser.add_argument('--epochs', default=50, type=int)\n",
        "parser.add_argument('--batch_size', default=103, type=int)\n",
        "parser.add_argument('--lr', default=0.01, type=float,\n",
        "                        help=\"Initial learning rate\")\n",
        "parser.add_argument('--lr_decay', default=0.9, type=float,\n",
        "                        help=\"The value multiplied by lr at each epoch. Set a larger value for larger epochs\")\n",
        "parser.add_argument('--lam_recon', default=0.392, type=float,\n",
        "                        help=\"The coefficient for the loss of decoder\")\n",
        "parser.add_argument('-r', '--routings', default=3, type=int,\n",
        "                        help=\"Number of iterations used in routing algorithm. should > 0\")\n",
        "parser.add_argument('--shift_fraction', default=0.1, type=float,\n",
        "                        help=\"Fraction of pixels to shift at most in each direction.\")\n",
        "parser.add_argument('--debug', action='store_true',\n",
        "                        help=\"Save weights by TensorBoard\")\n",
        "parser.add_argument('--save_dir', default='./result')\n",
        "parser.add_argument('-t', '--testing', action='store_true',\n",
        "                        help=\"Test the trained model on testing dataset\")\n",
        "parser.add_argument('--digit', default=5, type=int,\n",
        "                        help=\"Digit to manipulate\")\n",
        "parser.add_argument('-w', '--weights', default=None,\n",
        "                        help=\"The path of the saved weights. Should be specified when testing\")\n",
        "args, unknown = parser.parse_known_args()\n",
        "print(args)\n",
        "\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs(args.save_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=103, debug=False, digit=5, epochs=50, lam_recon=0.392, lr=0.01, lr_decay=0.9, routings=3, save_dir='./result', shift_fraction=0.1, testing=False, weights=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO123_3gudvD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "c0bff0b8-c142-4d76-c500-578d65cb4239"
      },
      "source": [
        "# define model\n",
        "\n",
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "    tf.keras.callbacks.History(),\n",
        "]\n",
        "\n",
        "model, eval_model, manipulate_model = CapsNet(input_shape=X_train.shape[1:],\n",
        "                                                  n_class=len(np.unique(np.argmax(y_train, 1))),\n",
        "                                                  routings=args.routings,\n",
        "                                                  batch_size=args.batch_size)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(103, 64, 64, 3)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (103, 32, 32, 3)     0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_conv2d (Conv2D)      (103, 10, 10, 256)   7168        average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_reshape (Reshape)    (103, 3200, 8)       0           primarycap_conv2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_squash (Lambda)      (103, 3200, 8)       0           primarycap_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "digitcaps (CapsuleLayer)        (103, 25, 16)        10240000    primarycap_squash[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 25)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "mask_3 (Mask)                   (103, 400)           0           digitcaps[0][0]                  \n",
            "                                                                 input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "capsnet (Length)                (103, 25)            0           digitcaps[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Sequential)            (None, 64, 64, 3)    13325824    mask_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 23,572,992\n",
            "Trainable params: 23,572,992\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC_uCQ3fuflr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "99afb2d3-46ce-471e-e028-12b2a0d957b8"
      },
      "source": [
        "history = train(model=model, data=((X_train, y_train), (X_test, y_test)), args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.5864 - capsnet_loss: 0.5495 - decoder_loss: 0.0942 - capsnet_accuracy: 0.3826\n",
            "Epoch 00001: capsnet_accuracy improved from -inf to 0.38264, saving model to ./result/weights-01.h5\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.5864 - capsnet_loss: 0.5495 - decoder_loss: 0.0942 - capsnet_accuracy: 0.3826 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.2039 - capsnet_loss: 0.1733 - decoder_loss: 0.0781 - capsnet_accuracy: 0.8141\n",
            "Epoch 00002: capsnet_accuracy improved from 0.38264 to 0.81411, saving model to ./result/weights-02.h5\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.2039 - capsnet_loss: 0.1733 - decoder_loss: 0.0781 - capsnet_accuracy: 0.8141 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1094 - capsnet_loss: 0.0807 - decoder_loss: 0.0732 - capsnet_accuracy: 0.9058\n",
            "Epoch 00003: capsnet_accuracy improved from 0.81411 to 0.90577, saving model to ./result/weights-03.h5\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.1094 - capsnet_loss: 0.0807 - decoder_loss: 0.0732 - capsnet_accuracy: 0.9058 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0875 - capsnet_loss: 0.0593 - decoder_loss: 0.0719 - capsnet_accuracy: 0.9389\n",
            "Epoch 00004: capsnet_accuracy improved from 0.90577 to 0.93889, saving model to ./result/weights-04.h5\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0875 - capsnet_loss: 0.0593 - decoder_loss: 0.0719 - capsnet_accuracy: 0.9389 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0714 - capsnet_loss: 0.0437 - decoder_loss: 0.0706 - capsnet_accuracy: 0.9585\n",
            "Epoch 00005: capsnet_accuracy improved from 0.93889 to 0.95845, saving model to ./result/weights-05.h5\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.0714 - capsnet_loss: 0.0437 - decoder_loss: 0.0706 - capsnet_accuracy: 0.9585 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0627 - capsnet_loss: 0.0353 - decoder_loss: 0.0701 - capsnet_accuracy: 0.9664\n",
            "Epoch 00006: capsnet_accuracy improved from 0.95845 to 0.96645, saving model to ./result/weights-06.h5\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.0627 - capsnet_loss: 0.0353 - decoder_loss: 0.0701 - capsnet_accuracy: 0.9664 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0585 - capsnet_loss: 0.0314 - decoder_loss: 0.0692 - capsnet_accuracy: 0.9737\n",
            "Epoch 00007: capsnet_accuracy improved from 0.96645 to 0.97373, saving model to ./result/weights-07.h5\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0585 - capsnet_loss: 0.0314 - decoder_loss: 0.0692 - capsnet_accuracy: 0.9737 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 8/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0538 - capsnet_loss: 0.0270 - decoder_loss: 0.0685 - capsnet_accuracy: 0.9789\n",
            "Epoch 00008: capsnet_accuracy improved from 0.97373 to 0.97887, saving model to ./result/weights-08.h5\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0538 - capsnet_loss: 0.0270 - decoder_loss: 0.0685 - capsnet_accuracy: 0.9789 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0499 - capsnet_loss: 0.0233 - decoder_loss: 0.0678 - capsnet_accuracy: 0.9819\n",
            "Epoch 00009: capsnet_accuracy improved from 0.97887 to 0.98187, saving model to ./result/weights-09.h5\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0499 - capsnet_loss: 0.0233 - decoder_loss: 0.0678 - capsnet_accuracy: 0.9819 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0477 - capsnet_loss: 0.0214 - decoder_loss: 0.0671 - capsnet_accuracy: 0.9834\n",
            "Epoch 00010: capsnet_accuracy improved from 0.98187 to 0.98344, saving model to ./result/weights-10.h5\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.0477 - capsnet_loss: 0.0214 - decoder_loss: 0.0671 - capsnet_accuracy: 0.9834 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0452 - capsnet_loss: 0.0191 - decoder_loss: 0.0666 - capsnet_accuracy: 0.9842\n",
            "Epoch 00011: capsnet_accuracy improved from 0.98344 to 0.98415, saving model to ./result/weights-11.h5\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.0452 - capsnet_loss: 0.0191 - decoder_loss: 0.0666 - capsnet_accuracy: 0.9842 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0434 - capsnet_loss: 0.0174 - decoder_loss: 0.0663 - capsnet_accuracy: 0.9854\n",
            "Epoch 00012: capsnet_accuracy improved from 0.98415 to 0.98544, saving model to ./result/weights-12.h5\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.0434 - capsnet_loss: 0.0174 - decoder_loss: 0.0663 - capsnet_accuracy: 0.9854 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0421 - capsnet_loss: 0.0162 - decoder_loss: 0.0661 - capsnet_accuracy: 0.9862\n",
            "Epoch 00013: capsnet_accuracy improved from 0.98544 to 0.98615, saving model to ./result/weights-13.h5\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.0421 - capsnet_loss: 0.0162 - decoder_loss: 0.0661 - capsnet_accuracy: 0.9862 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0412 - capsnet_loss: 0.0154 - decoder_loss: 0.0659 - capsnet_accuracy: 0.9860\n",
            "Epoch 00014: capsnet_accuracy did not improve from 0.98615\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0412 - capsnet_loss: 0.0154 - decoder_loss: 0.0659 - capsnet_accuracy: 0.9860 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0396 - capsnet_loss: 0.0139 - decoder_loss: 0.0657 - capsnet_accuracy: 0.9869\n",
            "Epoch 00015: capsnet_accuracy improved from 0.98615 to 0.98686, saving model to ./result/weights-15.h5\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.0396 - capsnet_loss: 0.0139 - decoder_loss: 0.0657 - capsnet_accuracy: 0.9869 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0387 - capsnet_loss: 0.0130 - decoder_loss: 0.0655 - capsnet_accuracy: 0.9881\n",
            "Epoch 00016: capsnet_accuracy improved from 0.98686 to 0.98815, saving model to ./result/weights-16.h5\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.0387 - capsnet_loss: 0.0130 - decoder_loss: 0.0655 - capsnet_accuracy: 0.9881 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0379 - capsnet_loss: 0.0123 - decoder_loss: 0.0654 - capsnet_accuracy: 0.9880\n",
            "Epoch 00017: capsnet_accuracy did not improve from 0.98815\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0379 - capsnet_loss: 0.0123 - decoder_loss: 0.0654 - capsnet_accuracy: 0.9880 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0376 - capsnet_loss: 0.0121 - decoder_loss: 0.0652 - capsnet_accuracy: 0.9884\n",
            "Epoch 00018: capsnet_accuracy improved from 0.98815 to 0.98844, saving model to ./result/weights-18.h5\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.0376 - capsnet_loss: 0.0121 - decoder_loss: 0.0652 - capsnet_accuracy: 0.9884 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 19/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0371 - capsnet_loss: 0.0116 - decoder_loss: 0.0651 - capsnet_accuracy: 0.9883\n",
            "Epoch 00019: capsnet_accuracy did not improve from 0.98844\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0371 - capsnet_loss: 0.0116 - decoder_loss: 0.0651 - capsnet_accuracy: 0.9883 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0366 - capsnet_loss: 0.0111 - decoder_loss: 0.0649 - capsnet_accuracy: 0.9883\n",
            "Epoch 00020: capsnet_accuracy did not improve from 0.98844\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0366 - capsnet_loss: 0.0111 - decoder_loss: 0.0649 - capsnet_accuracy: 0.9883 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0362 - capsnet_loss: 0.0108 - decoder_loss: 0.0648 - capsnet_accuracy: 0.9884\n",
            "Epoch 00021: capsnet_accuracy did not improve from 0.98844\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0362 - capsnet_loss: 0.0108 - decoder_loss: 0.0648 - capsnet_accuracy: 0.9884 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0361 - capsnet_loss: 0.0107 - decoder_loss: 0.0647 - capsnet_accuracy: 0.9884\n",
            "Epoch 00022: capsnet_accuracy did not improve from 0.98844\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0361 - capsnet_loss: 0.0107 - decoder_loss: 0.0647 - capsnet_accuracy: 0.9884 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0361 - capsnet_loss: 0.0108 - decoder_loss: 0.0646 - capsnet_accuracy: 0.9884\n",
            "Epoch 00023: capsnet_accuracy did not improve from 0.98844\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.0361 - capsnet_loss: 0.0108 - decoder_loss: 0.0646 - capsnet_accuracy: 0.9884 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0357 - capsnet_loss: 0.0104 - decoder_loss: 0.0645 - capsnet_accuracy: 0.9883\n",
            "Epoch 00024: capsnet_accuracy did not improve from 0.98844\n",
            "68/68 [==============================] - 184s 3s/step - loss: 0.0357 - capsnet_loss: 0.0104 - decoder_loss: 0.0645 - capsnet_accuracy: 0.9883 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 25/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0354 - capsnet_loss: 0.0102 - decoder_loss: 0.0644 - capsnet_accuracy: 0.9884\n",
            "Epoch 00025: capsnet_accuracy did not improve from 0.98844\n",
            "68/68 [==============================] - 184s 3s/step - loss: 0.0354 - capsnet_loss: 0.0102 - decoder_loss: 0.0644 - capsnet_accuracy: 0.9884 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 26/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0353 - capsnet_loss: 0.0101 - decoder_loss: 0.0643 - capsnet_accuracy: 0.9886\n",
            "Epoch 00026: capsnet_accuracy improved from 0.98844 to 0.98858, saving model to ./result/weights-26.h5\n",
            "68/68 [==============================] - 184s 3s/step - loss: 0.0353 - capsnet_loss: 0.0101 - decoder_loss: 0.0643 - capsnet_accuracy: 0.9886 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 27/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0351 - capsnet_loss: 0.0099 - decoder_loss: 0.0643 - capsnet_accuracy: 0.9887\n",
            "Epoch 00027: capsnet_accuracy improved from 0.98858 to 0.98872, saving model to ./result/weights-27.h5\n",
            "68/68 [==============================] - 184s 3s/step - loss: 0.0351 - capsnet_loss: 0.0099 - decoder_loss: 0.0643 - capsnet_accuracy: 0.9887 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0350 - capsnet_loss: 0.0098 - decoder_loss: 0.0642 - capsnet_accuracy: 0.9887\n",
            "Epoch 00028: capsnet_accuracy did not improve from 0.98872\n",
            "68/68 [==============================] - 183s 3s/step - loss: 0.0350 - capsnet_loss: 0.0098 - decoder_loss: 0.0642 - capsnet_accuracy: 0.9887 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 29/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0350 - capsnet_loss: 0.0098 - decoder_loss: 0.0642 - capsnet_accuracy: 0.9887\n",
            "Epoch 00029: capsnet_accuracy did not improve from 0.98872\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0350 - capsnet_loss: 0.0098 - decoder_loss: 0.0642 - capsnet_accuracy: 0.9887 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0349 - capsnet_loss: 0.0098 - decoder_loss: 0.0641 - capsnet_accuracy: 0.9887\n",
            "Epoch 00030: capsnet_accuracy did not improve from 0.98872\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0349 - capsnet_loss: 0.0098 - decoder_loss: 0.0641 - capsnet_accuracy: 0.9887 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 31/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0348 - capsnet_loss: 0.0097 - decoder_loss: 0.0641 - capsnet_accuracy: 0.9887\n",
            "Epoch 00031: capsnet_accuracy did not improve from 0.98872\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0348 - capsnet_loss: 0.0097 - decoder_loss: 0.0641 - capsnet_accuracy: 0.9887 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 32/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0348 - capsnet_loss: 0.0097 - decoder_loss: 0.0640 - capsnet_accuracy: 0.9887\n",
            "Epoch 00032: capsnet_accuracy did not improve from 0.98872\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0348 - capsnet_loss: 0.0097 - decoder_loss: 0.0640 - capsnet_accuracy: 0.9887 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 33/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0347 - capsnet_loss: 0.0096 - decoder_loss: 0.0640 - capsnet_accuracy: 0.9887\n",
            "Epoch 00033: capsnet_accuracy did not improve from 0.98872\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0347 - capsnet_loss: 0.0096 - decoder_loss: 0.0640 - capsnet_accuracy: 0.9887 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 34/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0346 - capsnet_loss: 0.0095 - decoder_loss: 0.0640 - capsnet_accuracy: 0.9887\n",
            "Epoch 00034: capsnet_accuracy did not improve from 0.98872\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0346 - capsnet_loss: 0.0095 - decoder_loss: 0.0640 - capsnet_accuracy: 0.9887 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 35/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0346 - capsnet_loss: 0.0095 - decoder_loss: 0.0639 - capsnet_accuracy: 0.9887\n",
            "Epoch 00035: capsnet_accuracy did not improve from 0.98872\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0346 - capsnet_loss: 0.0095 - decoder_loss: 0.0639 - capsnet_accuracy: 0.9887 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0345 - capsnet_loss: 0.0095 - decoder_loss: 0.0639 - capsnet_accuracy: 0.9887\n",
            "Epoch 00036: capsnet_accuracy did not improve from 0.98872\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0345 - capsnet_loss: 0.0095 - decoder_loss: 0.0639 - capsnet_accuracy: 0.9887 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0345 - capsnet_loss: 0.0095 - decoder_loss: 0.0639 - capsnet_accuracy: 0.9887\n",
            "Epoch 00037: capsnet_accuracy did not improve from 0.98872\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0345 - capsnet_loss: 0.0095 - decoder_loss: 0.0639 - capsnet_accuracy: 0.9887 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 38/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0345 - capsnet_loss: 0.0094 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889\n",
            "Epoch 00038: capsnet_accuracy improved from 0.98872 to 0.98886, saving model to ./result/weights-38.h5\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0345 - capsnet_loss: 0.0094 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 39/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0344 - capsnet_loss: 0.0094 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889\n",
            "Epoch 00039: capsnet_accuracy did not improve from 0.98886\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0344 - capsnet_loss: 0.0094 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 40/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0344 - capsnet_loss: 0.0094 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889\n",
            "Epoch 00040: capsnet_accuracy did not improve from 0.98886\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0344 - capsnet_loss: 0.0094 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0344 - capsnet_loss: 0.0094 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889\n",
            "Epoch 00041: capsnet_accuracy did not improve from 0.98886\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0344 - capsnet_loss: 0.0094 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0344 - capsnet_loss: 0.0094 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889\n",
            "Epoch 00042: capsnet_accuracy did not improve from 0.98886\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0344 - capsnet_loss: 0.0094 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889\n",
            "Epoch 00043: capsnet_accuracy did not improve from 0.98886\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0638 - capsnet_accuracy: 0.9889 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 44/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9889\n",
            "Epoch 00044: capsnet_accuracy did not improve from 0.98886\n",
            "68/68 [==============================] - 181s 3s/step - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9889 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 45/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0343 - capsnet_loss: 0.0094 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9889\n",
            "Epoch 00045: capsnet_accuracy did not improve from 0.98886\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0343 - capsnet_loss: 0.0094 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9889 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 46/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9889\n",
            "Epoch 00046: capsnet_accuracy did not improve from 0.98886\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9889 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9890\n",
            "Epoch 00047: capsnet_accuracy improved from 0.98886 to 0.98901, saving model to ./result/weights-47.h5\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9890 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 48/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9890\n",
            "Epoch 00048: capsnet_accuracy did not improve from 0.98901\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9890 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9890\n",
            "Epoch 00049: capsnet_accuracy did not improve from 0.98901\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0343 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9890 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Epoch 50/50\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0342 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9890\n",
            "Epoch 00050: capsnet_accuracy did not improve from 0.98901\n",
            "68/68 [==============================] - 182s 3s/step - loss: 0.0342 - capsnet_loss: 0.0093 - decoder_loss: 0.0637 - capsnet_accuracy: 0.9890 - val_loss: 0.0000e+00 - val_capsnet_loss: 0.0000e+00 - val_decoder_loss: 0.0000e+00 - val_capsnet_accuracy: 0.0000e+00\n",
            "Trained model saved to './result/trained_model.h5'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkNh5Qk9j7OD"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STz31BTKj2sM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6e823f39-c012-42e4-dfb9-8752fa2b5bde"
      },
      "source": [
        "# https://github.com/ishaandey/Classification_Evaluation_Walkthrough/blob/master/Classification_Evaluation.ipynb\n",
        "# All  import statements needed for the notebook\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from inspect import signature\n",
        "\n",
        "import sklearn\n",
        "from sklearn import linear_model, dummy, metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtbf4l-Oj8zn"
      },
      "source": [
        "def test(model, data, args):\n",
        "    X_test, y_test = data\n",
        "    y_pred, x_recon = model.predict(X_test, batch_size=103)\n",
        "    print('-' * 30 + 'Begin: test' + '-' * 30)\n",
        "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1)) / y_test.shape[0])\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRBi_vSSkwDl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1c554ecb-df2e-4071-ffaa-92e9765edf3d"
      },
      "source": [
        "test(model=eval_model, data=(X_test, y_test), args=args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------Begin: test------------------------------\n",
            "Test acc: 0.909973521624007\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}